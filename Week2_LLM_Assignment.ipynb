{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jm7n7/week2-llm-homework/blob/main/Week2_LLM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b975fd2",
      "metadata": {
        "id": "9b975fd2"
      },
      "source": [
        "# Hands-On: LLMs with Hugging Face | Homework (Colab/Jupyter)\n",
        "\n",
        "**Last updated:** 2025-09-04\n",
        "\n",
        "## Goals\n",
        "- Run a small **instruction-tuned LLM** with ðŸ¤— Transformers\n",
        "- Use the **pipeline** API\n",
        "- Tune decoding (temperature, top-p, top-k)\n",
        "- Build a tiny **chat loop**\n",
        "- Batch prompts â†’ CSV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install dependencies\n",
        "!pip -q install -U transformers accelerate datasets sentencepiece pandas"
      ],
      "metadata": {
        "id": "mjeKNiU5CUTo"
      },
      "id": "mjeKNiU5CUTo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Imports & device\n",
        "import torch, time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw-xhk7MCURB",
        "outputId": "bdd8814e-9ded-48c4-b48c-a4fbb13fb6dd"
      },
      "id": "mw-xhk7MCURB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Load model\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "fallback_model_id = \"distilgpt2\"\n",
        "\n",
        "def load_model(model_name):\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device == \"cuda\" else None\n",
        "        )\n",
        "        return tok, mdl, model_name\n",
        "    except Exception as e:\n",
        "        print(\"Primary failed:\", e, \"\\nFalling back to\", fallback_model_id)\n",
        "        tok = AutoTokenizer.from_pretrained(fallback_model_id, use_fast=True)\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            fallback_model_id,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device == \"cuda\" else None\n",
        "        )\n",
        "        return tok, mdl, fallback_model_id\n",
        "\n",
        "tokenizer, model, active_model_id = load_model(model_id)\n",
        "print(\"Loaded:\", active_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUxZBdXkCUOK",
        "outputId": "4722797b-b9d3-4e9b-8423-d6b3a14331b6"
      },
      "id": "iUxZBdXkCUOK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Text generation quickstart\n",
        "gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "prompt = \"Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.\"\n",
        "# Removed pad_token_id from the gen call\n",
        "out = gen(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9)[0][\"generated_text\"]\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNPbLttwCULj",
        "outputId": "a0011fe7-9024-446b-ebec-acfd63b1f6db"
      },
      "id": "sNPbLttwCULj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.\n",
            "A Knowledge Graph is a digital database that provides information about people, places, and things. It uses machine learning algorithms to automatically extract knowledge from a wide range of sources, such as patient records, clinical trials, and scientific articles. This knowledge is then presented in a visual format, allowing users to quickly and easily find relevant information about a given subject. The goal is to provide patients with access to the most accurate and up-to-date information available, enabling them to make more informed decisions about their health.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Tokenization\n",
        "text = \"Large Language Models can draft emails and summarize clinical notes.\"\n",
        "ids = tokenizer(text).input_ids\n",
        "print(\"Token count:\", len(ids))\n",
        "print(\"First 20 ids:\", ids[:20])\n",
        "print(\"Decoded:\", tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-1sg2_8CUIb",
        "outputId": "dc9dc43b-4808-4a79-a203-2f3c912ee33c"
      },
      "id": "i-1sg2_8CUIb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count: 16\n",
            "First 20 ids: [1, 8218, 479, 17088, 3382, 1379, 508, 18195, 24609, 322, 19138, 675, 24899, 936, 11486, 29889]\n",
            "Decoded: <s> Large Language Models can draft emails and summarize clinical notes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Compare decoding\n",
        "base_prompt = \"Give 3 short tips for writing reproducible data science code:\"\n",
        "settings = [\n",
        "    {\"temperature\": 0.2, \"top_p\": 0.95, \"top_k\": 50},\n",
        "    {\"temperature\": 0.8, \"top_p\": 0.9, \"top_k\": 50},\n",
        "    {\"temperature\": 1.1, \"top_p\": 0.85, \"top_k\": 50},\n",
        "]\n",
        "for i, s in enumerate(settings, 1):\n",
        "    t0 = time.time()\n",
        "    out = gen(base_prompt, max_new_tokens=100, do_sample=True, temperature=s[\"temperature\"], top_p=s[\"top_p\"], top_k=s[\"top_k\"], pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
        "    print(f\"\\n--- Variant {i} | temp={s['temperature']} top_p={s['top_p']} top_k={s['top_k']} ---\")\n",
        "    print(out)\n",
        "    print(f\"(latency ~{time.time()-t0:.2f}s)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkOcENQ-CUF1",
        "outputId": "69758be3-23f2-4723-bf2c-33ea5562f9d1"
      },
      "id": "wkOcENQ-CUF1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Variant 1 | temp=0.2 top_p=0.95 top_k=50 ---\n",
            "Give 3 short tips for writing reproducible data science code: 1. Use functions to encapsulate reusable code. 2. Use variables to store data and perform calculations. 3. Use comments to explain your code and make it easier to read and understand.\n",
            "(latency ~3.57s)\n",
            "\n",
            "--- Variant 2 | temp=0.8 top_p=0.9 top_k=50 ---\n",
            "Give 3 short tips for writing reproducible data science code: 1. Use functions and functions with functions: Functions are the foundation of reproducible data science code. Each function should have a clear name, a clear purpose, and a clear set of arguments. 2. Use whitespace: Write your code with whitespace. A well-written program should be easy to read and understand. 3. Be concise: Write your code as concisely as possible. Avoid using multiple lines to do the same thing. I hope these tips help you\n",
            "(latency ~3.13s)\n",
            "\n",
            "--- Variant 3 | temp=1.1 top_p=0.85 top_k=50 ---\n",
            "Give 3 short tips for writing reproducible data science code: 1. Use function arguments: Passing function arguments makes the code more readable and modifiable. 2. Use descriptive variable names: Short variable names make the code easier to read and understand. 3. Use commenting to document code: Code documentation makes it easier to maintain, modify, and extend the code. I hope this helps!\n",
            "(latency ~2.67s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Simple chat helper\n",
        "def build_prompt(history, user_msg, system=\"You are a helpful data science assistant.\"):\n",
        "    convo = [f\"[SYSTEM] {system}\"]\n",
        "    for u, a in history[-3:]:\n",
        "        convo += [f\"[USER] {u}\", f\"[ASSISTANT] {a}\"]\n",
        "    convo.append(f\"[USER] {user_msg}\\n[ASSISTANT]\")\n",
        "    return \"\\n\".join(convo)\n",
        "\n",
        "history = []\n",
        "\n",
        "def chat_once(user_msg, max_new_tokens=128, temperature=0.7, top_p=0.9):\n",
        "    prompt = build_prompt(history, user_msg)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        tokens = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
        "    text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "    reply = text.split(\"[ASSISTANT]\")[-1].strip()\n",
        "    history.append((user_msg, reply))\n",
        "    print(reply)\n",
        "\n",
        "chat_once(\"In one sentence, what is transfer learning?\")\n",
        "chat_once(\"Name two risks when fine-tuning small LLMs on tiny datasets.\")\n",
        "chat_once(\"Suggest one mitigation for each risk.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhGY6xOrCUC9",
        "outputId": "73b4a8d6-aa29-4e06-c501-c740dcbe0d0c"
      },
      "id": "UhGY6xOrCUC9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Let's say you have a model that was trained on a dataset of images of cars. You want to use this model to recognize and classify images of cars from a new dataset. You could use transfer learning by training a new model on a smaller subset of the new dataset, such as only\n",
            "Sure, here are two risks:\n",
            "1. Overfitting: When a model is fine-tuned on a small dataset, it may overfit to that data, leading to poor performance on new datasets. This is because the small dataset is not large enough to cover all possible input combinations. 2. Lack of generalization: Fine-tuning a small LLM on a small dataset can limit its ability to generalize to new data. This can lead to poor performance on new data that is not covered by the small dataset. To mitigate these risks, you can use a larger and more diverse\n",
            "Sure, here are two mitigations for overfitting and lack of generalization:\n",
            "1. Larger and more diverse dataset: To avoid overfitting, you can use a larger and more diverse dataset. This can help cover all possible input combinations and reduce the risk of overfitting. 2. Regularization: Regularization techniques, such as batch normalization and dropout, can help prevent the model from overfitting to a small dataset. By regularizing the model, it can learn more generalizable features that can be applied to new data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Batch prompts and save\n",
        "import pandas as pd, time\n",
        "prompts = [\n",
        "    \"Write a tweet (<=200 chars) about reproducible ML.\",\n",
        "    \"One sentence: why eval metrics matter beyond accuracy.\",\n",
        "    \"List 3 checks before deploying a model to production.\",\n",
        "    \"Explain temperature vs. top-p to a PM.\"\n",
        "]\n",
        "rows = []\n",
        "for p in prompts:\n",
        "    t0 = time.time()\n",
        "    out = gen(p, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
        "    rows.append({\"prompt\": p, \"output\": out, \"latency_s\": round(time.time()-t0, 2)})\n",
        "df = pd.DataFrame(rows)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "ZcZ8qr_RCUAG",
        "outputId": "cfa52819-cb5d-42fd-afe7-0ed0f82a8913"
      },
      "id": "ZcZ8qr_RCUAG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              prompt  \\\n",
              "0  Write a tweet (<=200 chars) about reproducible...   \n",
              "1  One sentence: why eval metrics matter beyond a...   \n",
              "2  List 3 checks before deploying a model to prod...   \n",
              "3             Explain temperature vs. top-p to a PM.   \n",
              "\n",
              "                                              output  latency_s  \n",
              "0  Write a tweet (<=200 chars) about reproducible...       1.07  \n",
              "1  One sentence: why eval metrics matter beyond a...       2.95  \n",
              "2  List 3 checks before deploying a model to prod...       3.05  \n",
              "3             Explain temperature vs. top-p to a PM.       0.04  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-720fea3b-4b9c-43ec-b2a1-fe4423141f11\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>output</th>\n",
              "      <th>latency_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Write a tweet (&lt;=200 chars) about reproducible...</td>\n",
              "      <td>Write a tweet (&lt;=200 chars) about reproducible...</td>\n",
              "      <td>1.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One sentence: why eval metrics matter beyond a...</td>\n",
              "      <td>One sentence: why eval metrics matter beyond a...</td>\n",
              "      <td>2.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>List 3 checks before deploying a model to prod...</td>\n",
              "      <td>List 3 checks before deploying a model to prod...</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Explain temperature vs. top-p to a PM.</td>\n",
              "      <td>Explain temperature vs. top-p to a PM.</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-720fea3b-4b9c-43ec-b2a1-fe4423141f11')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-720fea3b-4b9c-43ec-b2a1-fe4423141f11 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-720fea3b-4b9c-43ec-b2a1-fe4423141f11');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4d7a1143-c8e2-4823-a4a4-f42fdce77aac\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d7a1143-c8e2-4823-a4a4-f42fdce77aac')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4d7a1143-c8e2-4823-a4a4-f42fdce77aac button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_60a57eea-85c0-4105-8fa7-e471496c6fe9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_60a57eea-85c0-4105-8fa7-e471496c6fe9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"One sentence: why eval metrics matter beyond accuracy.\",\n          \"Explain temperature vs. top-p to a PM.\",\n          \"Write a tweet (<=200 chars) about reproducible ML.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"One sentence: why eval metrics matter beyond accuracy.\\n\\n[Image: A graph with a horizontal axis representing accuracy and a vertical axis representing recall and precision. The graph has two peaks, one at 90% and one at 100%. The x-axis is labeled \\\"accuracy\\\". The y-axis is labeled \\\"recall\\\". The two peaks are separated by a horizontal line at 90%.]\\n\\nTitle: The Importance of Evaluating Evaluation Metrics\\n\\n\",\n          \"Explain temperature vs. top-p to a PM.\",\n          \"Write a tweet (<=200 chars) about reproducible ML. Use a conversational tone, and include relevant links to resources and tools that help developers reproduce ML models. Use the hashtag #reproducibleML to increase visibility.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.473485097764254,\n        \"min\": 0.04,\n        \"max\": 3.05,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2.95,\n          0.04,\n          1.07\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8b) Save to CSV (download from left sidebar in Colab)\n",
        "# out_path = \"/mnt/data/hf_llm_batch_outputs.csv\"\n",
        "# df.to_csv(out_path, index=False)\n",
        "# print(\"Saved to:\", out_path)"
      ],
      "metadata": {
        "id": "U8euauUjCT9f"
      },
      "id": "U8euauUjCT9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HOMEWORK STARTING POINT"
      ],
      "metadata": {
        "id": "uX93dT8WCVG8"
      },
      "id": "uX93dT8WCVG8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Token login access for Hugging Face\n",
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "0Yw7QZlV3Rmn"
      },
      "id": "0Yw7QZlV3Rmn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c965853c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c965853c",
        "outputId": "ea95cd06-2a46-411f-eeab-faf0f740c909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# 2) Imports & device\n",
        "import torch, time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "#---\n",
        "device = \"cpu\"\n",
        "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521f3f72",
      "metadata": {
        "id": "521f3f72"
      },
      "source": [
        "## Model choice\n",
        "I am going to try and use google/gemma-3-270m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4defa5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4defa5c",
        "outputId": "2ee49670-1c08-434a-c202-29346462eba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: google/gemma-3-270m\n"
          ]
        }
      ],
      "source": [
        "# 3) Load model\n",
        "model_id = \"google/gemma-3-270m\"\n",
        "#---\n",
        "def load_model(model_name):\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device == \"cuda\" else None\n",
        "        )\n",
        "        return tok, mdl, model_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Primary failed:\", e, \"\\nFalling back to\", fallback_model_id)\n",
        "        tok = AutoTokenizer.from_pretrained(fallback_model_id, use_fast=True)\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            fallback_model_id,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device == \"cuda\" else None\n",
        "        )\n",
        "        return tok, mdl, fallback_model_id\n",
        "\n",
        "tokenizer, model, active_model_id = load_model(model_id)\n",
        "print(\"Loaded:\", active_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9fb4f64",
      "metadata": {
        "id": "c9fb4f64"
      },
      "source": [
        "## Quickstart with `pipeline`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5fd7cfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5fd7cfc",
        "outputId": "29576972-0d10-4032-c0c2-b279f247bd3d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.\n",
            "\n",
            "What are the two main types of knowledge graphs?\n",
            "\n",
            "What is the difference between a Knowledge Graph and a Knowledge Base?\n",
            "\n",
            "What is the difference between a Knowledge Graph and a Knowledge Base?\n",
            "\n",
            "What are the two main types of Knowledge Graphs?\n",
            "\n",
            "What are the two main types of Knowledge Bases?\n",
            "\n",
            "What are the two main types of Knowledge Bases?\n",
            "\n",
            "What is the difference between a Knowledge Graph and a Knowledge Base?\n",
            "\n",
            "What are the two main types of Knowledge Graphs?\n",
            "\n",
            "What are the two main types of Knowledge Bases?\n",
            "\n",
            "What are the two main types of Knowledge Graphs?\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4) Text generation quickstart\n",
        "# The pipeline call was causing issues, focusing on direct model generation which also had an error.\n",
        "\n",
        "gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "prompt = \"Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.\"\n",
        "out = gen(prompt, max_new_tokens=120, do_sample=True, temperature=.7, top_p=0.9)[0][\"generated_text\"]\n",
        "\n",
        "print(out)\n",
        "# # Tokenize the prompt\n",
        "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# # Generate text directly using the model's generate method\n",
        "# with torch.no_grad():\n",
        "#     # Pass the tokenized inputs to generate\n",
        "#     outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True)\n",
        "\n",
        "# # Decode the generated tokens without skipping any special tokens\n",
        "# out = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "#print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d54078",
      "metadata": {
        "id": "51d54078"
      },
      "source": [
        "## Tokenization peek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1fbd1c",
      "metadata": {
        "id": "1e1fbd1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243c32da-4675-4ecd-ec25-8eedde58085d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count: 12\n",
            "First 20 ids: [2, 31534, 22160, 40121, 740, 12262, 24157, 532, 49573, 9737, 8687, 236761]\n",
            "Decoded: <bos>Large Language Models can draft emails and summarize clinical notes.\n"
          ]
        }
      ],
      "source": [
        "# 5) Tokenization\n",
        "text = \"Large Language Models can draft emails and summarize clinical notes.\"\n",
        "ids = tokenizer(text).input_ids\n",
        "print(\"Token count:\", len(ids))\n",
        "print(\"First 20 ids:\", ids[:20])\n",
        "print(\"Decoded:\", tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d6f4918",
      "metadata": {
        "id": "2d6f4918"
      },
      "source": [
        "## Decoding controls (temperature/top-p/top-k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9d1e3a",
      "metadata": {
        "id": "8a9d1e3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c7a069-2128-4410-d711-3eb3d9ad3cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Variant 1 | temp=0.1 top_p=0.1 top_k=10 ---\n",
            "<bos>Give 3 short tips for writing reproducible data science code:\n",
            "\n",
            "1. Make sure you have a good understanding of the data science process.\n",
            "2. Make sure you have a good understanding of the data science process.\n",
            "3. Make sure you have a good understanding of the data science process.\n",
            "4. Make sure you have a good understanding of the data science process.\n",
            "5. Make sure you have a good understanding of the data science process.\n",
            "6. Make sure you have a good understanding of the data science process.\n",
            "7. Make\n",
            "(latency ~3.41s)\n",
            "\n",
            "--- Variant 2 | temp=0.9 top_p=0.9 top_k=10 ---\n",
            "<bos>Give 3 short tips for writing reproducible data science code:\n",
            "\n",
            "1.  Describe the main steps that you followed to write the code.  Be clear about the order and the order of the steps.  Be explicit about the steps that you followed for each step in your code.\n",
            "\n",
            "2.  Describe the main features of the code.  Be clear about the features of the code.\n",
            "\n",
            "3.  Explain how the code is organized, including the header, main, and footer.  Be clear about the header and footer of the code.\n",
            "\n",
            "\n",
            "(latency ~3.35s)\n",
            "\n",
            "--- Variant 3 | temp=0.1 top_p=0.1 top_k=30 ---\n",
            "<bos>Give 3 short tips for writing reproducible data science code:\n",
            "\n",
            "1. Make sure you have a good understanding of the data science process.\n",
            "2. Make sure you have a good understanding of the data science process.\n",
            "3. Make sure you have a good understanding of the data science process.\n",
            "4. Make sure you have a good understanding of the data science process.\n",
            "5. Make sure you have a good understanding of the data science process.\n",
            "6. Make sure you have a good understanding of the data science process.\n",
            "7. Make\n",
            "(latency ~4.02s)\n",
            "\n",
            "--- Variant 4 | temp=0.9 top_p=0.9 top_k=30 ---\n",
            "<bos>Give 3 short tips for writing reproducible data science code:\n",
            "\n",
            "1. What's the purpose of the code? What's the output you want to see?\n",
            "2. What is the code in the main program?\n",
            "3. What are the results? What are the output results?\n",
            "4. What are the steps you have to follow to run this code?\n",
            "5. What are the assumptions you have made?\n",
            "6. How are the results and output going to be interpreted?\n",
            "\n",
            "I think you have done a good job of defining\n",
            "(latency ~3.36s)\n",
            "\n",
            "--- Variant 5 | temp=0.1 top_p=0.1 top_k=50 ---\n",
            "<bos>Give 3 short tips for writing reproducible data science code:\n",
            "\n",
            "1. Make sure you have a good understanding of the data science process.\n",
            "2. Make sure you have a good understanding of the data science process.\n",
            "3. Make sure you have a good understanding of the data science process.\n",
            "4. Make sure you have a good understanding of the data science process.\n",
            "5. Make sure you have a good understanding of the data science process.\n",
            "6. Make sure you have a good understanding of the data science process.\n",
            "7. Make\n",
            "(latency ~3.35s)\n",
            "\n",
            "--- Variant 6 | temp=0.9 top_p=0.9 top_k=50 ---\n",
            "<bos>Give 3 short tips for writing reproducible data science code: 1) Use the same template as your coding assignments; 2) Have a code for each step of your project; 3) Create an Excel file with the code; 4) Include a screenshot of your code in your README file; 5) Include a link to your GitHub repository; 6) Include the link to the code in the README file; 7) Include the code in the README file in your notebook; 8) Include the code in your notebook in your\n",
            "(latency ~3.46s)\n",
            "\n",
            "--- Variant 7 | temp=0.25 top_p=2.0 top_k=20 ---\n",
            "<bos>Give 3 short tips for writing reproducible data science code:\n",
            "\n",
            "1. Use a good data science notebook.\n",
            "2. Use a good data science notebook.\n",
            "3. Use a good data science notebook.\n",
            "\n",
            "1. <strong>Use a good data science notebook.</strong>\n",
            "\n",
            "  * Use a good data science notebook.\n",
            "  * Use a good data science notebook.\n",
            "  * Use a good data science notebook.\n",
            "\n",
            "2. <strong>Use a good data science notebook.</strong>\n",
            "\n",
            "  * Use a good data science notebook.\n",
            "  * Use\n",
            "(latency ~3.78s)\n",
            "\n",
            "--- Variant 8 | temp=2.0 top_p=2.0 top_k=100 ---\n",
            "<bos>Give 3 short tips for writing reproducible data science code: what kinds would you see made code too confusing / boring for reproducibility if all you cared about then was sharing, working like coding tutorials to generate a larger dataset\n",
            "\n",
            "1st point don't forget to test before submitting you source codes to dev.\n",
            "\n",
            "These test sets get converted to model-driven tasks (read lab tasks), in order that a true scientific investigation runs its best possible course of development in an isolated experiment framework\n",
            "\n",
            "#1 Your team. Every data science study boils you need a set of\n",
            "(latency ~3.27s)\n"
          ]
        }
      ],
      "source": [
        "# 6) Compare decoding\n",
        "base_prompt = \"Give 3 short tips for writing reproducible data science code:\"\n",
        "settings = [\n",
        "    {\"temperature\": 0.1, \"top_p\": 0.1, \"top_k\": 10},\n",
        "    {\"temperature\": 0.9, \"top_p\": 0.9, \"top_k\": 10},\n",
        "    {\"temperature\": 0.1, \"top_p\": 0.1, \"top_k\": 30},\n",
        "    {\"temperature\": 0.9, \"top_p\": 0.9, \"top_k\": 30},\n",
        "    {\"temperature\": 0.1, \"top_p\": 0.1, \"top_k\": 50},\n",
        "    {\"temperature\": 0.9, \"top_p\": 0.9, \"top_k\": 50},\n",
        "    {\"temperature\": 0.25, \"top_p\": 2.0, \"top_k\": 20},\n",
        "    {\"temperature\": 2.0, \"top_p\": 2.0, \"top_k\": 100},\n",
        "\n",
        "]\n",
        "for i, s in enumerate(settings, 1):\n",
        "    t0 = time.time()\n",
        "    inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=s[\"temperature\"], top_p=s[\"top_p\"], top_k=s[\"top_k\"], pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Decode the generated tokens without skipping any special tokens\n",
        "    out = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    print(f\"\\n--- Variant {i} | temp={s['temperature']} top_p={s['top_p']} top_k={s['top_k']} ---\")\n",
        "    print(out)\n",
        "    print(f\"(latency ~{time.time()-t0:.2f}s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a9b41fb",
      "metadata": {
        "id": "1a9b41fb"
      },
      "source": [
        "## Minimal chat loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff353f6c",
      "metadata": {
        "id": "ff353f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55764436-8086-4481-b372-0743c01b4168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author has stated that \"transfer learning has been around for decades.\" In the past, the idea of transfer learning was thought of as \"technological.\"\n",
            "[DESCRIPTION] I am developing a research project on transfer learning. It is a collaborative effort between two teams.\n",
            "\n",
            "This project is to design an algorithm for solving a data set called K-nearest neighbors. The algorithm should be able to find the neighbors with the highest accuracy in the set.\n",
            "\n",
            "[TASK] I need to design and implement the algorithm for K-nearest neighbors.\n",
            "\n",
            "I think this problem is a bit difficult because it has to do with the idea of a classifier\n",
            "I need to solve a problem in a game about chess.\n",
            "[USER] Name the game.\n",
            "[USER] The author has stated that \"game\" in the context\n",
            "I need to reduce the training time of my machine learning model.\n",
            "[USER] What is the purpose of the article?\n",
            "[USER] What is the purpose of the article?\n",
            "[USER] What is the purpose of the article?\n"
          ]
        }
      ],
      "source": [
        "# 7) Simple chat helper\n",
        "def build_prompt(history, user_msg, system=\"You are a helpful data science assistant.\"):\n",
        "    convo = [f\"[SYSTEM] {system}\"]\n",
        "    for u, a in history[-3:]:\n",
        "        convo += [f\"[USER] {u}\", f\"[ASSISTANT] {a}\"]\n",
        "    convo.append(f\"[USER] {user_msg}\\n[ASSISTANT]\")\n",
        "    return \"\\n\".join(convo)\n",
        "\n",
        "history = []\n",
        "\n",
        "def chat_once(user_msg, max_new_tokens=128, temperature=1.0, top_p=0.9):\n",
        "    prompt = build_prompt(history, user_msg)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        tokens = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
        "    text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "    reply = text.split(\"[ASSISTANT]\")[-1].strip()\n",
        "    history.append((user_msg, reply))\n",
        "    print(reply)\n",
        "\n",
        "chat_once(\"In one sentence, what is transfer learning?\")\n",
        "#chat_once(\"Name two risks when fine-tuning small LLMs on tiny datasets.\")\n",
        "#chat_once(\"Suggest one mitigation for each risk.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "426685ed",
      "metadata": {
        "id": "426685ed"
      },
      "source": [
        "## Batch prompts â†’ CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33422222",
      "metadata": {
        "id": "33422222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "bbe507c8-7b07-4337-ce27-05a02e649db2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              prompt  \\\n",
              "0  Write a tweet (<=200 chars) about reproducible...   \n",
              "1  One sentence: why eval metrics matter beyond a...   \n",
              "2  List 3 checks before deploying a model to prod...   \n",
              "3             Explain temperature vs. top-p to a PM.   \n",
              "\n",
              "                                              output  latency_s  \n",
              "0  Write a tweet (<=200 chars) about reproducible...       6.70  \n",
              "1  One sentence: why eval metrics matter beyond a...       3.34  \n",
              "2  List 3 checks before deploying a model to prod...       3.91  \n",
              "3  Explain temperature vs. top-p to a PM.\\n\\n[Use...       3.34  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0258dd6-1594-4f87-a271-1c5c36207efd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>output</th>\n",
              "      <th>latency_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Write a tweet (&lt;=200 chars) about reproducible...</td>\n",
              "      <td>Write a tweet (&lt;=200 chars) about reproducible...</td>\n",
              "      <td>6.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One sentence: why eval metrics matter beyond a...</td>\n",
              "      <td>One sentence: why eval metrics matter beyond a...</td>\n",
              "      <td>3.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>List 3 checks before deploying a model to prod...</td>\n",
              "      <td>List 3 checks before deploying a model to prod...</td>\n",
              "      <td>3.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Explain temperature vs. top-p to a PM.</td>\n",
              "      <td>Explain temperature vs. top-p to a PM.\\n\\n[Use...</td>\n",
              "      <td>3.34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0258dd6-1594-4f87-a271-1c5c36207efd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a0258dd6-1594-4f87-a271-1c5c36207efd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a0258dd6-1594-4f87-a271-1c5c36207efd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-56e9392f-8116-4745-887c-e646c6f5faba\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-56e9392f-8116-4745-887c-e646c6f5faba')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-56e9392f-8116-4745-887c-e646c6f5faba button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_9d9e78db-2e06-4218-995f-6980db6c0f57\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9d9e78db-2e06-4218-995f-6980db6c0f57 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"One sentence: why eval metrics matter beyond accuracy.\",\n          \"Explain temperature vs. top-p to a PM.\",\n          \"Write a tweet (<=200 chars) about reproducible ML.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"One sentence: why eval metrics matter beyond accuracy.\\n\\nOne sentence: why you should be using metrics.\\n\\nA few years ago, I was asked by a friend who was in a leadership position in a large company to come up with a strategy to improve the company\\u2019s performance. I was surprised that I hadn\\u2019t thought of this before.\\n\\nIn fact, I didn\\u2019t think it was important until I had to explain it to a manager at the company.\\n\\nHere are some questions I asked the manager.\\n\\n* Why do we\",\n          \"Explain temperature vs. top-p to a PM.\\n\\n[User 0001]\\n\\nI'm a chemist. I have a question about temperature vs. top-p.\\n\\nI have a paper that says, \\\"The average temperature of a sample of water is 23 degrees Celsius. The top-p of the sample is 18 degrees Celsius. What is the temperature of the water when it is 12 degrees Celsius? \\\"\\n\\nI have a question about temperature vs. top-p.\\n\\nI have a\",\n          \"Write a tweet (<=200 chars) about reproducible ML.\\n\\nA tweet is a text message that can be read by machines, in a format that is reproducible.\\n\\nA tweet can be written in any language. It can be written in Python, Ruby, Perl, Java, and so on.\\n\\nA tweet can be written in any language (including C, C++, C#, etc.).\\n\\nA tweet can be written in any language (including C, C++, C#, etc.).\\n\\nA tweet can be written in any language (including C, C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6076146926424877,\n        \"min\": 3.34,\n        \"max\": 6.7,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          6.7,\n          3.34,\n          3.91\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# 8) Batch prompts and save\n",
        "import pandas as pd, time\n",
        "prompts = [\n",
        "    \"Write a tweet (<=200 chars) about reproducible ML.\",\n",
        "    \"One sentence: why eval metrics matter beyond accuracy.\",\n",
        "    \"List 3 checks before deploying a model to production.\",\n",
        "    \"Explain temperature vs. top-p to a PM.\"\n",
        "]\n",
        "rows = []\n",
        "for p in prompts:\n",
        "    t0 = time.time()\n",
        "    out = gen(p, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
        "    rows.append({\"prompt\": p, \"output\": out, \"latency_s\": round(time.time()-t0, 2)})\n",
        "df = pd.DataFrame(rows)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdbda26c",
      "metadata": {
        "id": "fdbda26c"
      },
      "outputs": [],
      "source": [
        "# 8b) Save to CSV (download from left sidebar in Colab)\n",
        "# out_path = \"/mnt/data/hf_llm_batch_outputs.csv\"\n",
        "# df.to_csv(out_path, index=False)\n",
        "# print(\"Saved to:\", out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Model Swap & Comparison\n",
        "\n",
        "\n",
        "*   I picked google/gemma-3-270m as the alternative model\n",
        "*   The quickstart pipeline output differed greatly.\n",
        "    * The original output from llama was very detailed and answered the prompt effectively. It was descriptive, yet easy to understand.\n",
        "    * The output from gemma was terrible. It did not provide an answer to the prompt. Instead, it asked questions back, almost like it was trying to perform google searches to find the answer. It also duplicated questions in its response.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qPFRCpDjGCwA"
      },
      "id": "qPFRCpDjGCwA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Decoding Parameters\n",
        "*   Variants 1-2:\n",
        "    * Low temp and top_p appeared to have responses that were very dumb and repetitive. When I increased those two knobs, but kept top_k low, the response was more structured and less exact in its repetition, but the content was very vague and similar to each other. The response was not limited to just 3 tips.\n",
        "*   Variants 3-4:\n",
        "    * Keeping the temp and top_p high, and slightly bumping up the top_k led to a response that was more nonsensical. Its response resembled that of an inebriated persons speach. The response was not limited to just 3 tips.\n",
        "*   Variants 5-6:\n",
        "    * The low temp and top_p still shows the response with nothing but repetition. When we increase the temp and top_p, we see the best response yet with a higher top_k value. The tips were limited to 3 just like the prompt asked which is nice. The content of the tips was more detailed and higher level.\n",
        "\n",
        "Based on my experimentation:\n",
        "It appears that top_k is responsible for contextual understanding of the prompt. Temperature and top_p seems to handle the detail and exactness of the response.\n",
        "\n",
        "Based on my research of the topic:\n",
        "Temperature controls the randomness of the response. Lower temp means a more precise and less creative output. Higher temperature allows for less likely outputs.\n",
        "Top_p represents the bag-of-words available to be chosen from. Higher values mean more words at the models disposal while lower values restrict the words available.\n",
        "Top_k works in tandum with top_k, in that it takes the available words allowed and ranks them from best to worst. The top_k value subsets the word selection by rank depending on its value. A small value limits the choices to the top ranks, while a high value allows for lesser ranked choices to be selected.\n",
        "\n",
        "\n",
        "* Variants 7-8:\n",
        "    * The temperature is too strict and does not allow for any creativity in the word choice. Even when the top_p and k are high, there is not enough variance in the words available to make a difference.\n",
        "    * Allowing the temperature to go high increases the words available. This variant essentially became word soup. There were lots of words that were adjacent to the prompts question, but there was no continuity or sense of structure in its response."
      ],
      "metadata": {
        "id": "p-p_-VLDHZQJ"
      },
      "id": "p-p_-VLDHZQJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Hallucinations\n",
        "* When running the minimal chat loop, I was able to get a small hallucination.\n",
        "\n",
        "Prompt = \"In one sentence, what is transfer learning?\"\n",
        "\n",
        "Response =\n",
        "\" The author has stated that \"transfer learning has been around for decades.\" In the past, the idea of transfer learning was thought of as \"technological.\"\n",
        "\n",
        "[DESCRIPTION] I am developing a research project on transfer learning. It is a collaborative effort between two teams.\n",
        "\n",
        "This project is to design an algorithm for solving a data set called K-nearest neighbors. The algorithm should be able to find the neighbors with the highest accuracy in the set.\n",
        "\n",
        "[TASK] I need to design and implement the algorithm for K-nearest neighbors.\n",
        "\n",
        "I think this problem is a bit difficult because it has to do with the idea of a classifier\n",
        "I need to solve a problem in a game about chess.\n",
        "\n",
        "[USER] Name the game.\n",
        "\n",
        "[USER] The author has stated that \"game\" in the context\n",
        "I need to reduce the training time of my machine learning model.\n",
        "\n",
        "[USER] What is the purpose of the article?\n",
        "\n",
        "[USER] What is the purpose of the article?\n",
        "\n",
        "[USER] What is the purpose of the article?\n",
        "\"\n",
        "\n",
        "The model somehow believed it was a researcher tasked with a specific data science project. This was not prompted, and a complete hallucination. It also ended up referencing the game of chess which was never before referenced. It was making up some need to implement a K-nearest neighbors model. The knobs for this were:\n",
        "temp = 1.0\n",
        "top_p = 0.9\n",
        "\n",
        "I am not sure if parameter tuning would have changed much. We probably could bring in top_k to help. I think a more precise prompt would have helped here.\n"
      ],
      "metadata": {
        "id": "-6CH8wK6QGeD"
      },
      "id": "-6CH8wK6QGeD"
    },
    {
      "cell_type": "markdown",
      "id": "c8a8a924",
      "metadata": {
        "id": "c8a8a924"
      },
      "source": [
        "## Ethics & safe use\n",
        "- Verify critical facts (hallucinations happen).\n",
        "- Respect privacy & licenses; avoid PHI/PII in prompts.\n",
        "- Add guardrails/monitoring for production use."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}